import os
import requests
from serpapi import GoogleSearch
from PIL import Image
from io import BytesIO
import open_clip
import torch
from tqdm import tqdm
from dotenv import load_dotenv
from supabase import create_client, Client
import time

load_dotenv()

supabase: Client = create_client(
    os.getenv('SUPABASE_URL'),
    os.getenv('SUPABASE_SERVICE_ROLE_KEY')
)

model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')
model = model.to('cuda' if torch.cuda.is_available() else 'cpu')

def search_images(query, num_results=100):
    params = {
        "engine": "google",
        "q": query,
        "tbm": "isch",
        "num": num_results,
        "api_key": os.getenv('SERPAPI_API_KEY')
    }
    search = GoogleSearch(params)
    results = search.get_dict()
    images = results.get('images_results', [])
    print(f"Found {len(images)} images for query: {query}")
    return images

def download_and_process_image(url):
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        image = Image.open(BytesIO(response.content)).convert('RGB')
        processed_image = preprocess(image).unsqueeze(0)
        return image, processed_image
    except Exception as e:
        print(f"Error processing image {url}: {str(e)}")
        return None, None

def filter_images(images, threshold=0.2):
    filtered_images = []
    text = torch.cat([
        open_clip.tokenize("a photo of a driveway"),
        open_clip.tokenize("a photo of asphalt"),
        open_clip.tokenize("a photo of a parking lot"),
        open_clip.tokenize("a photo of a patio"),
        open_clip.tokenize("a photo of an urban rooftop"),
        open_clip.tokenize("a photo of a swimming pool"),
    ])
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Using device: {device}")
    model.to(device)
    text = text.to(device)
    
    for img_data in tqdm(images, desc="Filtering images"):
        try:
            image, processed = download_and_process_image(img_data['original'])
            if image is None or processed is None:
                print(f"Skipping image {img_data['original']}: Could not process")
                continue

            with torch.no_grad():
                processed = processed.to(device)
                image_features = model.encode_image(processed)
                text_features = model.encode_text(text)
                similarity = (image_features @ text_features.T).squeeze(0)
                max_similarity = similarity.max().item()
                print(f"Image similarity score: {max_similarity}")

                if max_similarity > threshold:
                    img_data['similarity_score'] = max_similarity
                    filtered_images.append(img_data)

                    try:
                        img_buffer = BytesIO()
                        image.save(img_buffer, format='JPEG')
                        img_buffer.seek(0)

                        # ‚Äî save locally for training/evaluation ‚Äî
                        os.makedirs("data/not_yard", exist_ok=True)
                        local_filename = f"not_yard_{int(time.time())}_{len(filtered_images)}.jpg"
                        local_path = os.path.join("data/not_yard", local_filename)
                        image.save(local_path, format="JPEG")

                        filename = local_filename

                        supabase.storage.from_('external-lawn-photos').upload(
                            path=filename,
                            file=img_buffer.getvalue(),
                            file_options={"content-type": "image/jpeg"}
                        )

                        print(f"‚úÖ Uploaded image to Supabase: {filename}")

                        res = supabase.table('external_photos').insert({
                            'url': img_data['original'],
                            'label': 'not_yard',
                            'clip_score': max_similarity,
                            'file_path': filename,
                            'source': 'SerpAPI',
                            'search_term': img_data.get('title', 'unknown')
                        }).execute()

                        if hasattr(res, 'error') and res.error:
                            print(f"‚ùå Metadata insert error: {res.error}")
                        else:
                            print("üìù Metadata saved.")

                    except Exception as e:
                        print(f"‚ùå Error uploading to Supabase: {str(e)}")

        except Exception as e:
            print(f"‚ùå Error processing image: {str(e)}")
            continue

    return filtered_images

def main():
    queries = [
        "driveway design",
        "asphalt paving",
        "parking lot",
        "patio furniture",
        "urban rooftop",
        "swimming pool"
    ]
    
    all_filtered_images = []
    
    for query in queries:
        print(f"\nüîç Processing query: {query}")
        images = search_images(query)
        filtered = filter_images(images)
        all_filtered_images.extend(filtered)
        time.sleep(2)
    
    print(f"\n‚úÖ Total filtered images: {len(all_filtered_images)}")

if __name__ == "__main__":
    main()
